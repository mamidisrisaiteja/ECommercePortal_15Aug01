name: ü§ñ MCP Integration Pipeline

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'MCP test mode'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - quick
          - codegen_only
          - validation_only
  
  push:
    branches: [ main ]
    paths:
      - 'automation_framework/mcp_integration.py'
      - 'automation_framework/playwright_mcp_runner.py'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  mcp_server_health:
    name: üîç MCP Server Health Check
    runs-on: ubuntu-latest
    
    steps:
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: üé≠ Install Playwright MCP Server
        run: |
          npm install -g @executeautomation/playwright-mcp-server
          npx playwright install chromium
      
      - name: üîç Health Check
        run: |
          echo "Testing MCP server installation and basic functionality..."
          
          # Test MCP server availability
          npm list -g @executeautomation/playwright-mcp-server
          
          # Test Playwright installation
          npx playwright --version
          
          echo "‚úÖ MCP server health check passed"

  mcp_integration_tests:
    name: ü§ñ MCP Integration Tests
    needs: mcp_server_health
    runs-on: ubuntu-latest
    
    steps:
      - name: üìÅ Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: üé≠ Install Dependencies
        run: |
          npm install -g @executeautomation/playwright-mcp-server
          pip install -r automation_framework/requirements.txt
          playwright install chromium firefox webkit
      
      - name: üß™ Test MCP Session Management
        run: |
          cd automation_framework
          python -c "
          from mcp_integration import PlaywrightMCPIntegration
          from utils.logger import Logger
          import json
          
          logger = Logger('MCP_Session_Test')
          mcp = PlaywrightMCPIntegration()
          
          # Test 1: Session lifecycle
          logger.info('Testing session lifecycle...')
          success = mcp.start_recording_session('session_test')
          assert success, 'Failed to start session'
          
          session_info = mcp.get_session_info()
          assert session_info['active'], 'Session not active'
          
          summary = mcp.end_recording_session()
          assert summary['status'] == 'completed', 'Session did not complete'
          
          logger.info('‚úÖ Session lifecycle test passed')
          
          # Test 2: Action recording
          logger.info('Testing action recording...')
          mcp.start_recording_session('action_test')
          
          mcp.record_action('navigate', value='https://www.saucedemo.com')
          mcp.record_action('fill', selector='[data-test=\"username\"]', value='test_user')
          mcp.record_action('click', selector='[data-test=\"login-button\"]')
          mcp.record_action('assert_visible', selector='.inventory_list')
          
          session_info = mcp.get_session_info()
          assert session_info['actions_count'] == 4, f'Expected 4 actions, got {session_info[\"actions_count\"]}'
          
          summary = mcp.end_recording_session()
          logger.info(f'‚úÖ Action recording test passed: {summary}')
          
          print('All MCP session tests completed successfully!')
          "
      
      - name: üîß Test Code Generation
        run: |
          cd automation_framework
          python -c "
          from mcp_integration import PlaywrightMCPIntegration
          from utils.logger import Logger
          
          logger = Logger('MCP_CodeGen_Test')
          mcp = PlaywrightMCPIntegration()
          
          # Start session and record actions
          mcp.start_recording_session('codegen_test')
          
          # Record a complete test scenario
          mcp.record_action('navigate', value='https://www.saucedemo.com')
          mcp.record_action('fill', selector='[data-test=\"username\"]', value='standard_user')
          mcp.record_action('fill', selector='[data-test=\"password\"]', value='secret_sauce')
          mcp.record_action('click', selector='[data-test=\"login-button\"]')
          mcp.record_action('assert_visible', selector='.inventory_list')
          mcp.record_action('click', selector='[data-test=\"add-to-cart-sauce-labs-backpack\"]')
          mcp.record_action('assert_text', selector='.shopping_cart_badge', value='1')
          
          # Test Python code generation
          python_code = mcp.generate_test_code('python')
          assert python_code, 'Failed to generate Python code'
          assert 'def test_' in python_code, 'Python code missing test function'
          assert 'page.goto' in python_code, 'Python code missing navigation'
          
          # Test TypeScript code generation
          typescript_code = mcp.generate_test_code('typescript')
          assert typescript_code, 'Failed to generate TypeScript code'
          assert 'test(' in typescript_code, 'TypeScript code missing test function'
          assert 'page.goto' in typescript_code, 'TypeScript code missing navigation'
          
          # Save generated tests
          python_file = mcp.save_generated_test(python_code, 'python')
          typescript_file = mcp.save_generated_test(typescript_code, 'typescript')
          
          assert python_file, 'Failed to save Python test'
          assert typescript_file, 'Failed to save TypeScript test'
          
          mcp.end_recording_session()
          
          logger.info('‚úÖ Code generation test passed')
          print(f'Generated files: {python_file}, {typescript_file}')
          "
      
      - name: üèÉ Test MCP Runner
        run: |
          cd automation_framework
          python -c "
          from playwright_mcp_runner import PlaywrightMCPRunner
          from utils.logger import Logger
          
          logger = Logger('MCP_Runner_Test')
          runner = PlaywrightMCPRunner()
          
          # Test listing generated tests
          tests = runner.list_generated_tests()
          logger.info(f'Found {len(tests)} generated tests: {[t[\"name\"] for t in tests]}')
          
          # Test cleanup functionality
          cleanup_result = runner.cleanup_generated_tests(older_than_days=0)
          logger.info(f'Cleanup result: {cleanup_result}')
          
          logger.info('‚úÖ MCP runner test passed')
          "
      
      - name: üîÑ Test Cross-Browser Code Generation
        if: github.event.inputs.test_mode != 'quick'
        run: |
          cd automation_framework
          python -c "
          from mcp_integration import PlaywrightMCPIntegration
          from utils.logger import Logger
          
          logger = Logger('MCP_CrossBrowser_Test')
          browsers = ['chromium', 'firefox', 'webkit']
          
          for browser in browsers:
              logger.info(f'Testing code generation for {browser}...')
              
              mcp = PlaywrightMCPIntegration()
              mcp.start_recording_session(f'{browser}_test')
              
              # Record browser-specific actions
              mcp.record_action('navigate', value='https://www.saucedemo.com')
              mcp.record_action('fill', selector='[data-test=\"username\"]', value='standard_user')
              mcp.record_action('click', selector='[data-test=\"login-button\"]')
              
              # Generate and save test
              code = mcp.generate_test_code('python')
              assert code, f'Failed to generate code for {browser}'
              
              file_path = mcp.save_generated_test(code, 'python')
              assert file_path, f'Failed to save test for {browser}'
              
              mcp.end_recording_session()
              logger.info(f'‚úÖ {browser} test generation completed')
          
          print('All cross-browser tests completed successfully!')
          "
      
      - name: üì¶ Upload Generated Tests
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mcp-generated-tests
          path: automation_framework/generated_tests/
          retention-days: 7
      
      - name: üìä Generate MCP Report
        if: always()
        run: |
          cd automation_framework
          python -c "
          import os
          import json
          from datetime import datetime
          from pathlib import Path
          
          # Collect information about generated tests
          generated_dir = Path('generated_tests')
          tests = []
          
          if generated_dir.exists():
              for file in generated_dir.glob('*'):
                  if file.is_file():
                      tests.append({
                          'name': file.name,
                          'size': file.stat().st_size,
                          'language': 'python' if file.suffix == '.py' else 'typescript',
                          'created': datetime.fromtimestamp(file.stat().st_ctime).isoformat()
                      })
          
          report = {
              'timestamp': datetime.now().isoformat(),
              'total_tests_generated': len(tests),
              'tests': tests,
              'mcp_server_status': 'healthy',
              'integration_status': 'successful'
          }
          
          with open('mcp_integration_report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          print(f'MCP Integration Report: {len(tests)} tests generated')
          "
      
      - name: üìã Upload MCP Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mcp-integration-report
          path: automation_framework/mcp_integration_report.json
          retention-days: 30

  mcp_performance_test:
    name: ‚ö° MCP Performance Test
    needs: mcp_integration_tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_mode == 'full'
    
    steps:
      - name: üìÅ Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: üé≠ Install Dependencies
        run: |
          npm install -g @executeautomation/playwright-mcp-server
          pip install -r automation_framework/requirements.txt
          playwright install chromium
      
      - name: ‚ö° Performance Test
        run: |
          cd automation_framework
          python -c "
          from mcp_integration import PlaywrightMCPIntegration
          from utils.logger import Logger
          import time
          
          logger = Logger('MCP_Performance_Test')
          
          # Test 1: Session creation performance
          start_time = time.time()
          mcp = PlaywrightMCPIntegration()
          
          for i in range(10):
              success = mcp.start_recording_session(f'perf_test_{i}')
              assert success, f'Failed to start session {i}'
              mcp.clear_session()
          
          session_time = time.time() - start_time
          logger.info(f'Session creation performance: {session_time:.2f}s for 10 sessions')
          
          # Test 2: Action recording performance
          start_time = time.time()
          mcp.start_recording_session('action_perf_test')
          
          for i in range(100):
              mcp.record_action('click', selector=f'button_{i}', value=f'test_{i}')
          
          action_time = time.time() - start_time
          logger.info(f'Action recording performance: {action_time:.2f}s for 100 actions')
          
          # Test 3: Code generation performance
          start_time = time.time()
          python_code = mcp.generate_test_code('python')
          typescript_code = mcp.generate_test_code('typescript')
          
          codegen_time = time.time() - start_time
          logger.info(f'Code generation performance: {codegen_time:.2f}s for 2 languages')
          
          mcp.end_recording_session()
          
          # Performance thresholds
          assert session_time < 5.0, f'Session creation too slow: {session_time}s'
          assert action_time < 2.0, f'Action recording too slow: {action_time}s'
          assert codegen_time < 3.0, f'Code generation too slow: {codegen_time}s'
          
          logger.info('‚úÖ All performance tests passed')
          
          print(f'''
          Performance Test Results:
          - Session Creation: {session_time:.2f}s (< 5.0s ‚úÖ)
          - Action Recording: {action_time:.2f}s (< 2.0s ‚úÖ)  
          - Code Generation: {codegen_time:.2f}s (< 3.0s ‚úÖ)
          ''')
          "

  notify_mcp_results:
    name: üì¢ Notify MCP Results
    needs: [mcp_server_health, mcp_integration_tests, mcp_performance_test]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: üìä Determine Status
        id: status
        run: |
          if [[ "${{ needs.mcp_server_health.result }}" == "success" && 
                "${{ needs.mcp_integration_tests.result }}" == "success" && 
                ("${{ needs.mcp_performance_test.result }}" == "success" || "${{ needs.mcp_performance_test.result }}" == "skipped") ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "emoji=‚úÖ" >> $GITHUB_OUTPUT
            echo "color=good" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "emoji=‚ùå" >> $GITHUB_OUTPUT
            echo "color=danger" >> $GITHUB_OUTPUT
          fi
      
      - name: üìã Create Summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # ü§ñ MCP Integration Pipeline Summary
          
          ## Status: ${{ steps.status.outputs.emoji }} ${{ steps.status.outputs.status }}
          
          **Test Results:**
          - **MCP Server Health**: ${{ needs.mcp_server_health.result }}
          - **Integration Tests**: ${{ needs.mcp_integration_tests.result }}
          - **Performance Tests**: ${{ needs.mcp_performance_test.result }}
          
          **Run Information:**
          - **Trigger**: ${{ github.event_name }}
          - **Test Mode**: ${{ github.event.inputs.test_mode || 'full' }}
          - **Run Number**: #${{ github.run_number }}
          
          **Features Tested:**
          - ‚úÖ MCP server installation and health
          - ‚úÖ Session lifecycle management
          - ‚úÖ Action recording capabilities
          - ‚úÖ Code generation (Python & TypeScript)
          - ‚úÖ Cross-browser compatibility
          - ‚úÖ Performance benchmarks
          
          ---
          *MCP Integration powered by Playwright MCP Server*
          EOF
